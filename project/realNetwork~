from pybrain.tools.shortcuts import buildNetwork
from pybrain.supervised.trainers import BackpropTrainer
from pybrain.datasets import SupervisedDataSet,UnsupervisedDataSet
from pybrain.structure import LinearLayer, SigmoidLayer, SoftmaxLayer

#get no of words from corpus and store in numInputNodes and numOutputNodes
numInputNodes = 10
numHiddenNodes = 5
numOutputNodes = 10

#get this from deepthi's code
ds = SupervisedDataSet(21, 21)
ds.addSample(map(int,'1 2 4 6 2 3 4 5 1 3 5 6 7 1 4 7 1 2 3 5 6'.split()),map(int,'1 2 5 6 2 4 4 5 1 2 5 6 7 1 4 6 1 2 3 3 6'.split()))
ds.addSample(map(int,'1 2 5 6 2 4 4 5 1 2 5 6 7 1 4 6 1 2 3 3 6'.split()),map(int,'1 3 5 7 2 4 6 7 1 3 5 6 7 1 4 6 1 2 2 3 7'.split()))

#create a network: Inner laye : Linear, Hiddenlayer : Sigmoid, Outputlayer: Softmax, Type = recurrent
net = buildNetwork(numInputNodes, numHiddenNodes, numOutputNodes, hiddenclass=SigmoidLayer, outclass=SoftmaxLayer,bias=True, recurrent=True)

#train the network
# learningRate = 0.1 ~ according to paper
# lrDecay = rate of change of learningRate in between iterations of the same epoch
# batchlearning = if set to true the parameters are changed only after completion of entire epoch
trainer = BackpropTrainer(net, ds,  learningrate=0.1, lrdecay=1,batchlearning=True )
error = trainer.train()

#test it on unsupervised dataset : validation
#use the network to predict next word for the validation dataset and calculate the log likelyhood
#If log-likelihood of validation data increases, training continues in new epoch. If no significant improvement is observed, learning rate Î± is halved
#at start of each new epoch

# Instead of using the above method of manually checking till convergence we can also use the below. 
trainer.trainUntilConvergence(validationProportion=0.25, verbose = True);

